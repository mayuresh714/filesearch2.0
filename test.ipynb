{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> test of download_model_function </h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/paraphrase-albert-small-v2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from work_with_model import download_and_save_model\n",
    "model = \"sentence-transformers/paraphrase-albert-small-v2\"\n",
    "\n",
    "model.replace(\"@\",\"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/paraphrase-MiniLM-L3-v2\n"
     ]
    }
   ],
   "source": [
    "from work_with_model import load_model_bin\n",
    "\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "# model_name=model.split(\"/\")[-1]\n",
    "print(model_name)\n",
    "obj = load_model_bin(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='models/sentence-transformers/paraphrase-MiniLM-L3-v2/tokenizer', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 384)\n",
       "       (token_type_embeddings): Embedding(2, 384)\n",
       "       (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-2): 3 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       " ),\n",
       " BertTokenizerFast(name_or_path='models/sentence-transformers/paraphrase-MiniLM-L3-v2/tokenizer', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from work_with_model import load_model_bin\n",
    "\n",
    "load_model_bin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from work_with_model import encode_model_bin\n",
    "import torch\n",
    "\n",
    "def encode_text(text):\n",
    "    model,tokenizer = load_model_bin()\n",
    "    print(model is None)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        embeddings = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encode_text(\u001b[39m\"\u001b[39;49m\u001b[39mnlp book\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mencode_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoding)\n\u001b[1;32m---> 17\u001b[0m     last_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39;49mhidden_states[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     18\u001b[0m     embeddings \u001b[39m=\u001b[39m last_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "encode_text(\"nlp book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class model:\n",
    "    a = 1\n",
    "    b = a+1\n",
    "\n",
    "p = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from work_with_model import transformer_ops as tops\n",
    "name = \"GPT_125M\"\n",
    "wom = tops(name=name)\n",
    "# name = \"sentence-transformers@paraphrase-albert-small-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "wom.setter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT_125M already exists no need to download again ...\n"
     ]
    }
   ],
   "source": [
    "wom.download_and_save_model_pickle(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = wom.encode_single_doc(\"the cat is strong \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.11646667e-01, -3.32449190e-02,  4.19502817e-02, -1.35372598e-02,\n",
       "        1.25394072e-02, -5.47454767e-02, -9.21722874e-03,  1.30647756e-02,\n",
       "        2.34391186e-02, -1.68136824e-02, -1.19445743e-02,  8.12968761e-02,\n",
       "       -1.99614093e-02,  5.88950329e-02,  8.64244904e-03,  1.01656780e-01,\n",
       "        1.35027573e-01, -3.15942317e-02, -4.71081622e-02,  7.98866674e-02,\n",
       "        3.33311632e-02,  3.95192169e-02,  7.63263740e-03, -9.72219463e-03,\n",
       "        6.06915988e-02,  1.23816967e-01, -1.88962836e-02,  4.94065601e-03,\n",
       "        6.64008688e-03,  6.44846410e-02,  1.70861203e-02,  7.19634220e-02,\n",
       "        3.16454098e-04,  6.44286796e-02, -5.19151688e-02, -6.88865557e-02,\n",
       "       -1.06475890e-01,  1.02746882e-01, -8.52695294e-03,  4.72451607e-03,\n",
       "        5.18697612e-02,  1.17117107e-01, -4.46294509e-02,  6.99138418e-02,\n",
       "        1.90051999e-02, -6.65071160e-02, -3.49321105e-02, -7.01009631e-02,\n",
       "        3.19888331e-02, -5.05961142e-02, -1.00078635e-01,  3.10247745e-02,\n",
       "        1.32996336e-01, -2.56043710e-02, -5.50764836e-02,  3.01378556e-02,\n",
       "        1.20702246e-02,  8.28199834e-02,  4.20596041e-02, -3.31045613e-02,\n",
       "       -2.16511101e-03, -6.82499036e-02,  3.16790044e-02,  1.13093499e-02,\n",
       "       -5.64656034e-02, -4.88886535e-02, -9.37774256e-02, -7.51370490e-02,\n",
       "       -3.64141613e-02, -2.13168077e-02, -1.03735849e-01,  2.45036315e-02,\n",
       "       -7.76476189e-02, -4.15145978e-02,  6.25284836e-02,  1.15436599e-01,\n",
       "        2.51162723e-02,  4.30452973e-02, -6.99455440e-02,  3.33437733e-02,\n",
       "        3.08021866e-02,  9.35121179e-02,  8.04537162e-02, -6.41360879e-02,\n",
       "       -1.11114241e-01, -5.66415563e-02, -5.35263121e-02,  3.60887460e-02,\n",
       "        6.12979149e-03,  6.15172125e-02, -8.12252164e-02, -4.95590381e-02,\n",
       "       -6.37360513e-02,  4.18670885e-02,  6.40928894e-02, -3.00175529e-02,\n",
       "        5.37920594e-02, -7.31872842e-02, -1.35191157e-01, -2.85348408e-02,\n",
       "       -2.34544855e-02, -1.05146058e-02, -5.81548549e-02, -6.79345280e-02,\n",
       "        1.91081818e-02, -3.69247682e-02,  6.83789281e-03,  5.70195802e-02,\n",
       "        3.42379399e-02,  8.63322616e-03,  9.33643728e-02, -7.09725022e-02,\n",
       "        8.48185569e-02,  2.40026955e-02, -1.27171233e-01,  8.98334477e-03,\n",
       "        3.94208636e-03, -4.16176356e-02, -8.26444328e-02,  2.69754454e-02,\n",
       "        9.93418470e-02, -1.06287956e-01,  6.80938363e-02,  1.32907100e-03,\n",
       "        3.15352753e-02,  6.33275658e-02,  3.33579592e-02, -1.00503653e-01,\n",
       "       -7.40121156e-02, -4.72289436e-02,  1.10124573e-01, -1.03970982e-01,\n",
       "        1.28171861e-01, -8.88795853e-02, -5.70564978e-02,  5.00116646e-02,\n",
       "       -7.17513412e-02, -1.89693924e-02, -1.59561187e-01,  8.05994347e-02,\n",
       "        7.66281858e-02, -1.67085320e-01,  6.08502775e-02, -6.05938658e-02,\n",
       "        1.43956952e-02,  5.20493314e-02, -9.66271535e-02, -5.51239550e-02,\n",
       "       -9.70706984e-04, -7.04500154e-02,  8.36809203e-02, -6.51678145e-02,\n",
       "       -2.90163271e-02,  9.00137573e-02, -4.12096716e-02,  3.86054767e-03,\n",
       "        4.77245674e-02,  2.99303681e-02,  2.43474133e-02, -5.51119559e-02,\n",
       "       -3.44429798e-02, -1.17871769e-01,  9.53350856e-04, -6.45937547e-02,\n",
       "        1.46366830e-03, -2.24929415e-02,  7.18003139e-02, -4.71942732e-03,\n",
       "       -2.21364871e-02, -4.39569540e-02, -1.39085308e-01, -4.22751866e-02,\n",
       "       -2.93502305e-02, -2.52238922e-02,  6.90225810e-02,  1.94465183e-02,\n",
       "       -2.89801043e-02,  1.46620683e-02,  1.77445300e-02,  7.82267302e-02,\n",
       "       -8.43886938e-03, -9.92457643e-02, -6.49029091e-02, -8.10589641e-03,\n",
       "        4.16578092e-02, -2.17519775e-02,  9.59004462e-02,  5.84459603e-02,\n",
       "        6.08352907e-02, -3.41157429e-02,  5.75316045e-03, -4.85843718e-02,\n",
       "        1.15513317e-02, -1.14310114e-02, -1.04391584e-02, -1.28186226e-01,\n",
       "       -5.31153791e-02,  3.66396690e-03,  3.12916748e-02, -6.21342994e-02,\n",
       "        2.02241801e-02,  1.30247459e-01,  4.52844016e-02,  4.78152633e-02,\n",
       "        1.89772751e-02, -2.29094480e-03, -5.55196106e-02, -4.17772047e-02,\n",
       "       -9.13467538e-03, -1.40280463e-02,  7.20143616e-02,  9.17598307e-02,\n",
       "       -3.43425525e-03, -1.35040164e-01, -1.19376160e-01,  8.31868798e-02,\n",
       "        6.00799546e-02,  6.96169883e-02, -4.05050144e-02, -2.30496228e-02,\n",
       "        1.36985183e-02,  4.81557399e-02,  6.58019483e-02, -7.66598508e-02,\n",
       "       -1.21855676e-01, -2.59756073e-02, -7.41339475e-02, -2.78803073e-02,\n",
       "       -7.58768842e-02,  2.40585878e-02, -5.28569780e-02,  7.49879852e-02,\n",
       "        6.28585462e-03,  9.72800553e-02,  2.69372948e-04,  3.43367495e-02,\n",
       "       -8.53807852e-02, -4.82794754e-02,  1.07098676e-01,  7.35999644e-02,\n",
       "       -6.82721063e-02,  1.08595930e-01,  3.31341513e-02,  4.91645299e-02,\n",
       "       -4.04122435e-02,  7.08219707e-02,  8.50269049e-02,  2.72409320e-02,\n",
       "        7.96061289e-03, -3.13910693e-02,  4.50871736e-02, -2.55446620e-02,\n",
       "        2.61909347e-02, -1.57034565e-02, -3.35996747e-02,  8.13268870e-02,\n",
       "        4.33727168e-02, -3.07833347e-02,  2.37764157e-02,  2.94890739e-02,\n",
       "        1.62404031e-05,  8.55038166e-02, -5.02822138e-02, -2.48593688e-02,\n",
       "       -6.87925592e-02,  4.38746214e-02,  1.25442207e-01, -8.46560150e-02,\n",
       "        7.84652680e-02, -8.40081125e-02, -4.42552865e-02,  3.44784819e-02,\n",
       "       -4.43305187e-02, -1.49903540e-02,  1.19673377e-02,  1.09814622e-01,\n",
       "       -2.30909865e-02, -1.32089928e-01,  1.13715336e-01, -7.81467184e-02,\n",
       "        2.88167223e-02,  3.18778865e-02,  7.60122687e-02, -2.74446849e-02,\n",
       "        2.66728867e-02, -6.89420616e-03, -6.19102046e-02,  3.87551412e-02,\n",
       "        1.99796967e-02,  1.17267966e-01,  3.36019136e-02, -3.92520763e-02,\n",
       "       -1.17159188e-01, -9.60866511e-02, -3.83950211e-02,  1.05282456e-01,\n",
       "        1.21678375e-01,  5.05110659e-02,  2.63560228e-02,  4.69711833e-02,\n",
       "       -4.68232706e-02,  4.32297103e-02,  6.03106953e-02, -1.18591130e-01,\n",
       "       -1.31051347e-01, -5.95336296e-02, -6.98249117e-02,  6.47922531e-02,\n",
       "        1.33491326e-02,  1.14526171e-02,  8.50754231e-02,  6.93323687e-02,\n",
       "       -1.69030093e-02,  3.78036383e-03, -4.42467909e-03, -8.64523649e-02,\n",
       "        5.97209707e-02, -1.13886066e-01,  2.77803130e-02, -4.41432595e-02,\n",
       "       -4.23058569e-02, -3.68130170e-02, -4.06966321e-02,  7.77864680e-02,\n",
       "        4.22588456e-03, -1.95784289e-02,  2.01349091e-02,  6.29771426e-02,\n",
       "       -1.84815079e-02, -7.31882676e-02,  8.00943971e-02, -3.04995794e-02,\n",
       "       -3.04557029e-02,  6.01690449e-02,  5.26915677e-02, -3.74632888e-02,\n",
       "        4.05575708e-03,  4.82753031e-02, -1.01894915e-01,  2.13353969e-02,\n",
       "        6.56875521e-02, -8.09732974e-02,  6.56765550e-02, -1.17528953e-01,\n",
       "        6.77064657e-02, -1.12560757e-01,  7.04092234e-02,  5.04568145e-02,\n",
       "        4.41235714e-02, -4.99915704e-02,  3.96153629e-02, -2.99761910e-02,\n",
       "        1.08715571e-01, -6.67374209e-02, -2.89661102e-02, -6.78007007e-02,\n",
       "       -9.06697065e-02,  3.34660225e-02,  7.32347667e-02,  9.29679424e-02,\n",
       "        2.29600142e-03, -5.63073158e-03, -2.65360717e-03, -7.72288069e-02,\n",
       "       -8.27292819e-03, -7.47842118e-02,  2.11459566e-02,  2.19731089e-02,\n",
       "        7.93036893e-02,  4.06621844e-02, -4.60349210e-02,  6.98653758e-02,\n",
       "        6.13824017e-02,  1.21910758e-02, -7.05417842e-02, -3.74939926e-02,\n",
       "       -9.76945758e-02, -1.54225817e-02, -1.10241231e-02,  1.30886701e-03,\n",
       "        9.29762051e-03,  3.64865288e-02, -1.71195164e-01, -3.98828462e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "from work_with_model import transformer_ops as tops\n",
    "name = \"GPT_125M\"\n",
    " \n",
    "wom = tops(name=name)\n",
    "wom.setter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'transformer_ops' object has no attribute 'access_model_for_tesing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer,model \u001b[39m=\u001b[39m wom\u001b[39m.\u001b[39;49maccess_model_for_tesing()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'transformer_ops' object has no attribute 'access_model_for_tesing'"
     ]
    }
   ],
   "source": [
    "tokenizer,model = wom.access_model_for_tesing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.LongTensor{[1, 6, 1]}, size=[1, 2]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[39mprint\u001b[39m(sentence_embeddings)\n\u001b[0;32m     35\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(sentence_embeddings[\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> 37\u001b[0m encode_into_vector(sentences)\n",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m, in \u001b[0;36mencode_into_vector\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     28\u001b[0m     model_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_input)\n\u001b[0;32m     30\u001b[0m \u001b[39m# Perform pooling. In this case, max pooling.\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m sentence_embeddings \u001b[39m=\u001b[39m mean_pooling(model_output, encoded_input[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSentence embeddings:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[39mprint\u001b[39m(sentence_embeddings)\n",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m, in \u001b[0;36mmean_pooling\u001b[1;34m(model_output, attention_mask)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_pooling\u001b[39m(model_output, attention_mask):\n\u001b[0;32m      7\u001b[0m     token_embeddings \u001b[39m=\u001b[39m model_output[\u001b[39m0\u001b[39m] \u001b[39m#First element of model_output contains all token embeddings\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     input_mask_expanded \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mexpand(token_embeddings\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msum(token_embeddings \u001b[39m*\u001b[39m input_mask_expanded, \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mclamp(input_mask_expanded\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m), \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1e-9\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.LongTensor{[1, 6, 1]}, size=[1, 2]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "sentences = [\"the cat is strong \"]\n",
    "\n",
    "\n",
    "# # Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\n",
    "\n",
    "\n",
    "def encode_into_vector(sentences):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    print(\"Sentence embeddings:\")\n",
    "    print(sentence_embeddings)\n",
    "    print(len(sentence_embeddings[0]))\n",
    "\n",
    "encode_into_vector(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
