{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> test of download_model_function </h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/paraphrase-albert-small-v2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from work_with_model import download_and_save_model\n",
    "model = \"sentence-transformers/paraphrase-albert-small-v2\"\n",
    "\n",
    "model.replace(\"@\",\"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/paraphrase-MiniLM-L3-v2\n"
     ]
    }
   ],
   "source": [
    "from work_with_model import load_model_bin\n",
    "\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "# model_name=model.split(\"/\")[-1]\n",
    "print(model_name)\n",
    "obj = load_model_bin(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='models/sentence-transformers/paraphrase-MiniLM-L3-v2/tokenizer', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 384)\n",
       "       (token_type_embeddings): Embedding(2, 384)\n",
       "       (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-2): 3 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       " ),\n",
       " BertTokenizerFast(name_or_path='models/sentence-transformers/paraphrase-MiniLM-L3-v2/tokenizer', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from work_with_model import load_model_bin\n",
    "\n",
    "load_model_bin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from work_with_model import encode_model_bin\n",
    "import torch\n",
    "\n",
    "def encode_text(text):\n",
    "    model,tokenizer = load_model_bin()\n",
    "    print(model is None)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        embeddings = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encode_text(\u001b[39m\"\u001b[39;49m\u001b[39mnlp book\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mencode_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoding)\n\u001b[1;32m---> 17\u001b[0m     last_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39;49mhidden_states[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     18\u001b[0m     embeddings \u001b[39m=\u001b[39m last_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "encode_text(\"nlp book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class model:\n",
    "    a = 1\n",
    "    b = a+1\n",
    "\n",
    "p = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from work_with_model import transformer_ops as tops\n",
    "name = \"GPT_125M\"\n",
    "wom = tops(name=name)\n",
    "# name = \"sentence-transformers@paraphrase-albert-small-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "wom.setter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT_125M already exists no need to download again ...\n"
     ]
    }
   ],
   "source": [
    "wom.download_and_save_model_pickle(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = wom.encode_single_doc(\"the cat is strong \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.11646667e-01, -3.32449190e-02,  4.19502817e-02, -1.35372598e-02,\n",
       "        1.25394072e-02, -5.47454767e-02, -9.21722874e-03,  1.30647756e-02,\n",
       "        2.34391186e-02, -1.68136824e-02, -1.19445743e-02,  8.12968761e-02,\n",
       "       -1.99614093e-02,  5.88950329e-02,  8.64244904e-03,  1.01656780e-01,\n",
       "        1.35027573e-01, -3.15942317e-02, -4.71081622e-02,  7.98866674e-02,\n",
       "        3.33311632e-02,  3.95192169e-02,  7.63263740e-03, -9.72219463e-03,\n",
       "        6.06915988e-02,  1.23816967e-01, -1.88962836e-02,  4.94065601e-03,\n",
       "        6.64008688e-03,  6.44846410e-02,  1.70861203e-02,  7.19634220e-02,\n",
       "        3.16454098e-04,  6.44286796e-02, -5.19151688e-02, -6.88865557e-02,\n",
       "       -1.06475890e-01,  1.02746882e-01, -8.52695294e-03,  4.72451607e-03,\n",
       "        5.18697612e-02,  1.17117107e-01, -4.46294509e-02,  6.99138418e-02,\n",
       "        1.90051999e-02, -6.65071160e-02, -3.49321105e-02, -7.01009631e-02,\n",
       "        3.19888331e-02, -5.05961142e-02, -1.00078635e-01,  3.10247745e-02,\n",
       "        1.32996336e-01, -2.56043710e-02, -5.50764836e-02,  3.01378556e-02,\n",
       "        1.20702246e-02,  8.28199834e-02,  4.20596041e-02, -3.31045613e-02,\n",
       "       -2.16511101e-03, -6.82499036e-02,  3.16790044e-02,  1.13093499e-02,\n",
       "       -5.64656034e-02, -4.88886535e-02, -9.37774256e-02, -7.51370490e-02,\n",
       "       -3.64141613e-02, -2.13168077e-02, -1.03735849e-01,  2.45036315e-02,\n",
       "       -7.76476189e-02, -4.15145978e-02,  6.25284836e-02,  1.15436599e-01,\n",
       "        2.51162723e-02,  4.30452973e-02, -6.99455440e-02,  3.33437733e-02,\n",
       "        3.08021866e-02,  9.35121179e-02,  8.04537162e-02, -6.41360879e-02,\n",
       "       -1.11114241e-01, -5.66415563e-02, -5.35263121e-02,  3.60887460e-02,\n",
       "        6.12979149e-03,  6.15172125e-02, -8.12252164e-02, -4.95590381e-02,\n",
       "       -6.37360513e-02,  4.18670885e-02,  6.40928894e-02, -3.00175529e-02,\n",
       "        5.37920594e-02, -7.31872842e-02, -1.35191157e-01, -2.85348408e-02,\n",
       "       -2.34544855e-02, -1.05146058e-02, -5.81548549e-02, -6.79345280e-02,\n",
       "        1.91081818e-02, -3.69247682e-02,  6.83789281e-03,  5.70195802e-02,\n",
       "        3.42379399e-02,  8.63322616e-03,  9.33643728e-02, -7.09725022e-02,\n",
       "        8.48185569e-02,  2.40026955e-02, -1.27171233e-01,  8.98334477e-03,\n",
       "        3.94208636e-03, -4.16176356e-02, -8.26444328e-02,  2.69754454e-02,\n",
       "        9.93418470e-02, -1.06287956e-01,  6.80938363e-02,  1.32907100e-03,\n",
       "        3.15352753e-02,  6.33275658e-02,  3.33579592e-02, -1.00503653e-01,\n",
       "       -7.40121156e-02, -4.72289436e-02,  1.10124573e-01, -1.03970982e-01,\n",
       "        1.28171861e-01, -8.88795853e-02, -5.70564978e-02,  5.00116646e-02,\n",
       "       -7.17513412e-02, -1.89693924e-02, -1.59561187e-01,  8.05994347e-02,\n",
       "        7.66281858e-02, -1.67085320e-01,  6.08502775e-02, -6.05938658e-02,\n",
       "        1.43956952e-02,  5.20493314e-02, -9.66271535e-02, -5.51239550e-02,\n",
       "       -9.70706984e-04, -7.04500154e-02,  8.36809203e-02, -6.51678145e-02,\n",
       "       -2.90163271e-02,  9.00137573e-02, -4.12096716e-02,  3.86054767e-03,\n",
       "        4.77245674e-02,  2.99303681e-02,  2.43474133e-02, -5.51119559e-02,\n",
       "       -3.44429798e-02, -1.17871769e-01,  9.53350856e-04, -6.45937547e-02,\n",
       "        1.46366830e-03, -2.24929415e-02,  7.18003139e-02, -4.71942732e-03,\n",
       "       -2.21364871e-02, -4.39569540e-02, -1.39085308e-01, -4.22751866e-02,\n",
       "       -2.93502305e-02, -2.52238922e-02,  6.90225810e-02,  1.94465183e-02,\n",
       "       -2.89801043e-02,  1.46620683e-02,  1.77445300e-02,  7.82267302e-02,\n",
       "       -8.43886938e-03, -9.92457643e-02, -6.49029091e-02, -8.10589641e-03,\n",
       "        4.16578092e-02, -2.17519775e-02,  9.59004462e-02,  5.84459603e-02,\n",
       "        6.08352907e-02, -3.41157429e-02,  5.75316045e-03, -4.85843718e-02,\n",
       "        1.15513317e-02, -1.14310114e-02, -1.04391584e-02, -1.28186226e-01,\n",
       "       -5.31153791e-02,  3.66396690e-03,  3.12916748e-02, -6.21342994e-02,\n",
       "        2.02241801e-02,  1.30247459e-01,  4.52844016e-02,  4.78152633e-02,\n",
       "        1.89772751e-02, -2.29094480e-03, -5.55196106e-02, -4.17772047e-02,\n",
       "       -9.13467538e-03, -1.40280463e-02,  7.20143616e-02,  9.17598307e-02,\n",
       "       -3.43425525e-03, -1.35040164e-01, -1.19376160e-01,  8.31868798e-02,\n",
       "        6.00799546e-02,  6.96169883e-02, -4.05050144e-02, -2.30496228e-02,\n",
       "        1.36985183e-02,  4.81557399e-02,  6.58019483e-02, -7.66598508e-02,\n",
       "       -1.21855676e-01, -2.59756073e-02, -7.41339475e-02, -2.78803073e-02,\n",
       "       -7.58768842e-02,  2.40585878e-02, -5.28569780e-02,  7.49879852e-02,\n",
       "        6.28585462e-03,  9.72800553e-02,  2.69372948e-04,  3.43367495e-02,\n",
       "       -8.53807852e-02, -4.82794754e-02,  1.07098676e-01,  7.35999644e-02,\n",
       "       -6.82721063e-02,  1.08595930e-01,  3.31341513e-02,  4.91645299e-02,\n",
       "       -4.04122435e-02,  7.08219707e-02,  8.50269049e-02,  2.72409320e-02,\n",
       "        7.96061289e-03, -3.13910693e-02,  4.50871736e-02, -2.55446620e-02,\n",
       "        2.61909347e-02, -1.57034565e-02, -3.35996747e-02,  8.13268870e-02,\n",
       "        4.33727168e-02, -3.07833347e-02,  2.37764157e-02,  2.94890739e-02,\n",
       "        1.62404031e-05,  8.55038166e-02, -5.02822138e-02, -2.48593688e-02,\n",
       "       -6.87925592e-02,  4.38746214e-02,  1.25442207e-01, -8.46560150e-02,\n",
       "        7.84652680e-02, -8.40081125e-02, -4.42552865e-02,  3.44784819e-02,\n",
       "       -4.43305187e-02, -1.49903540e-02,  1.19673377e-02,  1.09814622e-01,\n",
       "       -2.30909865e-02, -1.32089928e-01,  1.13715336e-01, -7.81467184e-02,\n",
       "        2.88167223e-02,  3.18778865e-02,  7.60122687e-02, -2.74446849e-02,\n",
       "        2.66728867e-02, -6.89420616e-03, -6.19102046e-02,  3.87551412e-02,\n",
       "        1.99796967e-02,  1.17267966e-01,  3.36019136e-02, -3.92520763e-02,\n",
       "       -1.17159188e-01, -9.60866511e-02, -3.83950211e-02,  1.05282456e-01,\n",
       "        1.21678375e-01,  5.05110659e-02,  2.63560228e-02,  4.69711833e-02,\n",
       "       -4.68232706e-02,  4.32297103e-02,  6.03106953e-02, -1.18591130e-01,\n",
       "       -1.31051347e-01, -5.95336296e-02, -6.98249117e-02,  6.47922531e-02,\n",
       "        1.33491326e-02,  1.14526171e-02,  8.50754231e-02,  6.93323687e-02,\n",
       "       -1.69030093e-02,  3.78036383e-03, -4.42467909e-03, -8.64523649e-02,\n",
       "        5.97209707e-02, -1.13886066e-01,  2.77803130e-02, -4.41432595e-02,\n",
       "       -4.23058569e-02, -3.68130170e-02, -4.06966321e-02,  7.77864680e-02,\n",
       "        4.22588456e-03, -1.95784289e-02,  2.01349091e-02,  6.29771426e-02,\n",
       "       -1.84815079e-02, -7.31882676e-02,  8.00943971e-02, -3.04995794e-02,\n",
       "       -3.04557029e-02,  6.01690449e-02,  5.26915677e-02, -3.74632888e-02,\n",
       "        4.05575708e-03,  4.82753031e-02, -1.01894915e-01,  2.13353969e-02,\n",
       "        6.56875521e-02, -8.09732974e-02,  6.56765550e-02, -1.17528953e-01,\n",
       "        6.77064657e-02, -1.12560757e-01,  7.04092234e-02,  5.04568145e-02,\n",
       "        4.41235714e-02, -4.99915704e-02,  3.96153629e-02, -2.99761910e-02,\n",
       "        1.08715571e-01, -6.67374209e-02, -2.89661102e-02, -6.78007007e-02,\n",
       "       -9.06697065e-02,  3.34660225e-02,  7.32347667e-02,  9.29679424e-02,\n",
       "        2.29600142e-03, -5.63073158e-03, -2.65360717e-03, -7.72288069e-02,\n",
       "       -8.27292819e-03, -7.47842118e-02,  2.11459566e-02,  2.19731089e-02,\n",
       "        7.93036893e-02,  4.06621844e-02, -4.60349210e-02,  6.98653758e-02,\n",
       "        6.13824017e-02,  1.21910758e-02, -7.05417842e-02, -3.74939926e-02,\n",
       "       -9.76945758e-02, -1.54225817e-02, -1.10241231e-02,  1.30886701e-03,\n",
       "        9.29762051e-03,  3.64865288e-02, -1.71195164e-01, -3.98828462e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding text function new way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915637195110321 Around 9 Million people live in London\n",
      "0.4947577714920044 London is known for its financial district\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n",
    "#Mean Pooling - Take average of all tokens\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "#Encode text\n",
    "def encode(texts):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "\n",
    "    # Perform pooling\n",
    "    embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "def check_transformers(string):\n",
    "    if hasattr(transformers, string):\n",
    "        return getattr(transformers, string)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "AutoModel     = check_transformers(\"AutoModel\")\n",
    "AutoTokenizer = check_transformers(\"AutoTokenizer\")\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "query = \"How many people live in London?\"\n",
    "docs = [\"Around 9 Million people live in London\", \"London is known for its financial district\"]\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\")\n",
    "\n",
    "#Encode query and docs\n",
    "query_emb = encode(query)\n",
    "doc_emb = encode(docs)\n",
    "\n",
    "#Compute dot score between query and all document embeddings\n",
    "scores = torch.mm(query_emb, doc_emb.transpose(0, 1))[0].cpu().tolist()\n",
    "\n",
    "#Combine docs & scores\n",
    "doc_score_pairs = list(zip(docs, scores))\n",
    "\n",
    "#Sort by decreasing score\n",
    "doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#Output passages & scores\n",
    "for doc, score in doc_score_pairs:\n",
    "    print(score, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists and model also downloaded\n"
     ]
    }
   ],
   "source": [
    "from work_with_model import transformer_ops as tops\n",
    "\n",
    "name = \"sentence-transformers@multi-qa-MiniLM-L6-cos-v1\"\n",
    "# name = \"GPT_125M\"\n",
    " \n",
    "wom = tops(name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer,model = wom.access_model_for_tesing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.auto.tokenization_auto.AutoTokenizer"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "def check_transformers(string):\n",
    "    if hasattr(transformers, string):\n",
    "        return getattr(transformers, string)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "check_transformers(\"AutoModel\")\n",
    "check_transformers(\"AutoTokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.LongTensor{[1, 6, 1]}, size=[1, 2]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[39mprint\u001b[39m(sentence_embeddings)\n\u001b[0;32m     35\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(sentence_embeddings[\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> 37\u001b[0m encode_into_vector(sentences)\n",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m, in \u001b[0;36mencode_into_vector\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m     28\u001b[0m     model_output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_input)\n\u001b[0;32m     30\u001b[0m \u001b[39m# Perform pooling. In this case, max pooling.\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m sentence_embeddings \u001b[39m=\u001b[39m mean_pooling(model_output, encoded_input[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSentence embeddings:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[39mprint\u001b[39m(sentence_embeddings)\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mmean_pooling\u001b[1;34m(model_output, attention_mask)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_pooling\u001b[39m(model_output, attention_mask):\n\u001b[0;32m      7\u001b[0m     token_embeddings \u001b[39m=\u001b[39m model_output[\u001b[39m0\u001b[39m] \u001b[39m#First element of model_output contains all token embeddings\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     input_mask_expanded \u001b[39m=\u001b[39m attention_mask\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mexpand(token_embeddings\u001b[39m.\u001b[39;49msize())\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msum(token_embeddings \u001b[39m*\u001b[39m input_mask_expanded, \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m torch\u001b[39m.\u001b[39mclamp(input_mask_expanded\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m), \u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1e-9\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.LongTensor{[1, 6, 1]}, size=[1, 2]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "sentences = [\"the cat is strong \"]\n",
    "\n",
    "\n",
    "# # Load model from HuggingFace Hub\n",
    "# tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\n",
    "# model = AutoModel.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\n",
    "\n",
    "\n",
    "def encode_into_vector(sentences):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    print(\"Sentence embeddings:\")\n",
    "    print(sentence_embeddings)\n",
    "    print(len(sentence_embeddings[0]))\n",
    "\n",
    "encode_into_vector(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp311-cp311-win_amd64.whl (23.9 MB)\n",
      "     ---------------------------------------- 0.0/23.9 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/23.9 MB 991.0 kB/s eta 0:00:25\n",
      "     ---------------------------------------- 0.1/23.9 MB 1.5 MB/s eta 0:00:16\n",
      "     --------------------------------------- 0.1/23.9 MB 901.1 kB/s eta 0:00:27\n",
      "     --------------------------------------- 0.2/23.9 MB 980.4 kB/s eta 0:00:25\n",
      "     ---------------------------------------- 0.3/23.9 MB 1.2 MB/s eta 0:00:21\n",
      "      --------------------------------------- 0.4/23.9 MB 1.4 MB/s eta 0:00:18\n",
      "      --------------------------------------- 0.5/23.9 MB 1.5 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 0.6/23.9 MB 1.5 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 0.7/23.9 MB 1.6 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 0.8/23.9 MB 1.7 MB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.9/23.9 MB 1.7 MB/s eta 0:00:14\n",
      "     - -------------------------------------- 1.0/23.9 MB 1.8 MB/s eta 0:00:13\n",
      "     - -------------------------------------- 1.2/23.9 MB 1.8 MB/s eta 0:00:13\n",
      "     -- ------------------------------------- 1.3/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 1.4/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 1.5/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 1.6/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 1.7/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 1.8/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 1.9/23.9 MB 2.0 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 1.9/23.9 MB 2.0 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 2.0/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 2.1/23.9 MB 1.9 MB/s eta 0:00:12\n",
      "     --- ------------------------------------ 2.3/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 2.4/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 2.5/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 2.6/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 2.7/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 2.7/23.9 MB 1.9 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 2.8/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 3.0/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 3.1/23.9 MB 2.0 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 3.2/23.9 MB 2.1 MB/s eta 0:00:11\n",
      "     ----- ---------------------------------- 3.4/23.9 MB 2.1 MB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 3.5/23.9 MB 2.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 3.7/23.9 MB 2.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 3.8/23.9 MB 2.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 3.9/23.9 MB 2.1 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 4.0/23.9 MB 2.2 MB/s eta 0:00:10\n",
      "     ------ --------------------------------- 4.1/23.9 MB 2.2 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 4.3/23.9 MB 2.1 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 4.3/23.9 MB 2.1 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 4.5/23.9 MB 2.2 MB/s eta 0:00:10\n",
      "     ------- -------------------------------- 4.5/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     ------- -------------------------------- 4.6/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     ------- -------------------------------- 4.7/23.9 MB 2.1 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 4.8/23.9 MB 2.1 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 5.0/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 5.0/23.9 MB 2.1 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 5.1/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 5.3/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     -------- ------------------------------- 5.4/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 5.5/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 5.6/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 5.7/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 5.8/23.9 MB 2.1 MB/s eta 0:00:09\n",
      "     --------- ------------------------------ 5.9/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 6.0/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 6.1/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 6.1/23.9 MB 2.1 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 6.4/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     ---------- ----------------------------- 6.5/23.9 MB 2.2 MB/s eta 0:00:09\n",
      "     ----------- ---------------------------- 6.6/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ----------- ---------------------------- 6.7/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ----------- ---------------------------- 6.9/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ----------- ---------------------------- 7.0/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ----------- ---------------------------- 7.1/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 7.3/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 7.5/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 7.6/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------ --------------------------- 7.8/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 7.9/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 8.0/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 8.2/23.9 MB 2.2 MB/s eta 0:00:08\n",
      "     ------------- -------------------------- 8.3/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     -------------- ------------------------- 8.5/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     -------------- ------------------------- 8.6/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     -------------- ------------------------- 8.8/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     -------------- ------------------------- 8.9/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     -------------- ------------------------- 9.0/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 9.0/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 9.2/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 9.3/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     --------------- ------------------------ 9.5/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 9.7/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 9.9/23.9 MB 2.3 MB/s eta 0:00:07\n",
      "     ---------------- ----------------------- 10.0/23.9 MB 2.3 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.2/23.9 MB 2.3 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.3/23.9 MB 2.4 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.5/23.9 MB 2.4 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.6/23.9 MB 2.4 MB/s eta 0:00:06\n",
      "     ----------------- ---------------------- 10.8/23.9 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 10.9/23.9 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.1/23.9 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.1/23.9 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.1/23.9 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------ --------------------- 11.4/23.9 MB 2.4 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 11.6/23.9 MB 2.5 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 11.7/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 11.8/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 11.9/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.0/23.9 MB 2.4 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.1/23.9 MB 2.4 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.2/23.9 MB 2.4 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.2/23.9 MB 2.4 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.3/23.9 MB 2.4 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 12.5/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 12.7/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 12.8/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 12.9/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 13.1/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 13.2/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 13.4/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 13.6/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 13.7/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 13.8/23.9 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 13.9/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 14.1/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 14.2/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 14.3/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 14.4/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 14.6/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 14.7/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 14.8/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 14.9/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 15.0/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 15.1/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 15.2/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 15.3/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 15.5/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 15.5/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 15.6/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 15.7/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 15.8/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 15.9/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 16.0/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 16.1/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 16.3/23.9 MB 2.5 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 16.4/23.9 MB 2.6 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 16.5/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 16.6/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 16.7/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 16.9/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 17.0/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 17.1/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 17.2/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 17.3/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 17.4/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 17.5/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 17.6/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 17.7/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 17.8/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 17.9/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 18.0/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 18.1/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 18.2/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 18.3/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 18.3/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 18.4/23.9 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 18.5/23.9 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 18.7/23.9 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 18.8/23.9 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 18.9/23.9 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 19.0/23.9 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 19.1/23.9 MB 2.4 MB/s eta 0:00:03\n",
      "     -------------------------------- ------- 19.2/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 19.3/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 19.4/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 19.5/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 19.7/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 19.8/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 19.9/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 20.0/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 20.1/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 20.2/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 20.3/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 20.4/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 20.5/23.9 MB 2.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 20.6/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 20.7/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 20.9/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 20.9/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 21.0/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 21.2/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 21.2/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 21.3/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 21.4/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 21.5/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 21.6/23.9 MB 2.3 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 21.7/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 21.8/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 21.9/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 22.0/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 22.1/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 22.2/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 22.3/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 22.4/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 22.5/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 22.6/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 22.7/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.8/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 22.9/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 23.0/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 23.1/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 23.2/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 23.3/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.4/23.9 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.5/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.6/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.6/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.7/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.8/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  23.9/23.9 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 23.9/23.9 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\shree\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.24.2)\n",
      "Collecting scipy>=1.7.0\n",
      "  Using cached scipy-1.10.1-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 0.0/56.8 kB ? eta -:--:--\n",
      "     ------------------------------------ --- 51.2/56.8 kB 2.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 56.8/56.8 kB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "Successfully installed gensim-4.3.1 scipy-1.10.1 smart-open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shree\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shree\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shree\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0002148   0.00935951  0.00167974  0.00904912 -0.00824874  0.00864258\n",
      "  0.00969176 -0.0079954   0.00134886  0.00818412  0.00885907 -0.00500215\n",
      " -0.00490618 -0.00961742  0.00423111 -0.002891    0.00361814  0.00456762\n",
      "  0.00208496 -0.00475434  0.00592477  0.00973361 -0.00728942  0.00339555\n",
      "  0.00282166  0.0060231   0.00781083  0.00559872 -0.00594684 -0.00517285\n",
      "  0.00821882 -0.0096197   0.00495436  0.00889858  0.00207776  0.00326364\n",
      "  0.00310452  0.00227818  0.00533976 -0.00350923  0.00437772 -0.00365946\n",
      " -0.00487915 -0.00158954  0.00970974 -0.00068267 -0.00135474 -0.00738559\n",
      "  0.00331212 -0.00108221]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import tokenize\n",
    "from file_metadata import get_all_metadata\n",
    "\n",
    "# define a list of documents\n",
    "documents = get_all_metadata\n",
    "\n",
    "# tokenize each document\n",
    "tokenized_docs = [ list(tokenize(doc.lower())) for doc in documents]\n",
    "\n",
    "# create a list of TaggedDocument objects\n",
    "tagged_docs = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(tokenized_docs)]\n",
    "\n",
    "# train a Doc2Vec model\n",
    "model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(tagged_docs)\n",
    "model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# get the document vector for the first document\n",
    "doc_vector = model.infer_vector(tokenized_docs[0])\n",
    "print(doc_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<new_sql.sql_ops at 0x129e47d4b90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import test\n",
    "\n",
    "test.db_obj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading cos-v1 sent transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists but model not downloaded\n"
     ]
    }
   ],
   "source": [
    "from work_with_model import transformer_ops\n",
    "\n",
    "name = \"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "\n",
    "trobj = transformer_ops(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "sentence-transformers@multi-qa-MiniLM-L6-cos-v1\n",
      "d:\\pendrive\\filesearch2.0\\models\\sentence-transformers@multi-qa-MiniLM-L6-cos-v1\\model.pkl\n",
      "d:\\pendrive\\filesearch2.0\\models\\sentence-transformers@multi-qa-MiniLM-L6-cos-v1\\tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "trobj.get_data_for_testing_purpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de732e5e3b134c8e989324a059a04f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shree\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895bb12be8b140dabf01bf9c0714efb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0158f84483254b2380fb2b40f58a42a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fb5319017d4f3ca550eb38ddd590c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a1b3bb47db41f989d734ca6c2d44e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983529b5fe294b0e8fe7edfa8ef57e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trobj.download_and_save_model_pickle(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
