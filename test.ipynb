{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> test of download_model_function </h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence-transformers/paraphrase-albert-small-v2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from work_with_model import download_and_save_model\n",
    "model = \"sentence-transformers/paraphrase-albert-small-v2\"\n",
    "\n",
    "model.replace(\"@\",\"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers/paraphrase-MiniLM-L3-v2\n"
     ]
    }
   ],
   "source": [
    "from work_with_model import load_model_bin\n",
    "\n",
    "model_name = \"sentence-transformers/paraphrase-MiniLM-L3-v2\"\n",
    "# model_name=model.split(\"/\")[-1]\n",
    "print(model_name)\n",
    "obj = load_model_bin(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='models/sentence-transformers/paraphrase-MiniLM-L3-v2/tokenizer', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 384)\n",
       "       (token_type_embeddings): Embedding(2, 384)\n",
       "       (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-2): 3 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "               (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "             (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       " ),\n",
       " BertTokenizerFast(name_or_path='models/sentence-transformers/paraphrase-MiniLM-L3-v2/tokenizer', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from work_with_model import load_model_bin\n",
    "\n",
    "load_model_bin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from work_with_model import encode_model_bin\n",
    "import torch\n",
    "\n",
    "def encode_text(text):\n",
    "    model,tokenizer = load_model_bin()\n",
    "    print(model is None)\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        embeddings = last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m encode_text(\u001b[39m\"\u001b[39;49m\u001b[39mnlp book\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m, in \u001b[0;36mencode_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoding)\n\u001b[1;32m---> 17\u001b[0m     last_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39;49mhidden_states[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     18\u001b[0m     embeddings \u001b[39m=\u001b[39m last_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "encode_text(\"nlp book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class model:\n",
    "    a = 1\n",
    "    b = a+1\n",
    "\n",
    "p = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from work_with_model import transformer_ops as tops\n",
    "name = \"GPT_125M\"\n",
    "wom = tops(name=name)\n",
    "# name = \"sentence-transformers@paraphrase-albert-small-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "wom.setter(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT_125M already exists no need to download again ...\n"
     ]
    }
   ],
   "source": [
    "wom.download_and_save_model_pickle(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = wom.encode_single_doc(\"the cat is strong and has eaten rat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08235384, -0.02042743,  0.06621283, -0.00539877,  0.04291169,\n",
       "       -0.05068491, -0.03325076,  0.0922316 ,  0.00734016,  0.01798994,\n",
       "       -0.0713305 ,  0.05039785, -0.01601499,  0.1057223 ,  0.02131992,\n",
       "        0.10951817,  0.04903874, -0.02486438,  0.01305457,  0.11004685,\n",
       "        0.03038722,  0.00421404,  0.03267882, -0.049477  ,  0.03422258,\n",
       "        0.10292463,  0.01380672,  0.06906645,  0.08547013,  0.10240253,\n",
       "        0.02345021,  0.06812023, -0.0618082 , -0.00498297, -0.05749562,\n",
       "       -0.07924826, -0.06724481,  0.05553465, -0.01387   ,  0.03890328,\n",
       "        0.03713493,  0.10886794, -0.07062145,  0.0373233 , -0.01620992,\n",
       "       -0.08771899, -0.11835511, -0.08457901,  0.01721183, -0.0514229 ,\n",
       "       -0.0351016 ,  0.03031924,  0.08902972, -0.0394065 , -0.1025086 ,\n",
       "       -0.0105591 ,  0.00085191,  0.00470831, -0.00350298, -0.03778702,\n",
       "       -0.01487388,  0.00477025,  0.02340586, -0.02283034,  0.01276157,\n",
       "       -0.11446246, -0.08243497, -0.13390358,  0.00978376,  0.00306894,\n",
       "       -0.10025796,  0.04827258, -0.0520542 , -0.00461618,  0.04222065,\n",
       "        0.06488124,  0.0054427 ,  0.07225593, -0.04235756,  0.02173794,\n",
       "        0.04278969,  0.05601559,  0.09492487, -0.05548641, -0.07705055,\n",
       "       -0.09140486, -0.02528321,  0.12834926, -0.05933866,  0.10214759,\n",
       "       -0.08623363, -0.03580345, -0.0680372 ,  0.05846044,  0.07969046,\n",
       "       -0.04098305,  0.01626845, -0.05622615, -0.10968102,  0.00722133,\n",
       "       -0.02887199, -0.02072677, -0.05517714, -0.09781328,  0.06053142,\n",
       "       -0.0198453 , -0.01516979,  0.07541338, -0.02380705,  0.02846802,\n",
       "        0.06266592, -0.06985985,  0.05717627,  0.03609955, -0.14187011,\n",
       "       -0.01910244, -0.00486265,  0.03505389, -0.13096309,  0.03655246,\n",
       "        0.0523017 , -0.11816607,  0.05915004, -0.03320519,  0.03966863,\n",
       "        0.07135097,  0.02721808, -0.06006579, -0.13772649, -0.08567172,\n",
       "        0.090878  , -0.04133307,  0.0518608 , -0.10665575, -0.13353816,\n",
       "        0.05559308, -0.03604227, -0.02527882, -0.12330442,  0.11344466,\n",
       "        0.08078233, -0.14782327,  0.03474683, -0.05674611,  0.0539116 ,\n",
       "        0.01432653, -0.09369618, -0.0760899 , -0.01434732, -0.13253017,\n",
       "        0.08393464, -0.03554032, -0.06275041,  0.05039208,  0.02013533,\n",
       "       -0.0327655 , -0.01027009,  0.0609334 ,  0.03396713, -0.10421088,\n",
       "       -0.03400353, -0.02012491,  0.02712721, -0.01098817, -0.02047682,\n",
       "       -0.01202873,  0.06806339, -0.04669996,  0.01408558, -0.07295065,\n",
       "       -0.09296105, -0.07742892, -0.02667263, -0.07796652,  0.04129828,\n",
       "        0.03543445,  0.02891319, -0.026573  , -0.01462893,  0.00031941,\n",
       "        0.0400366 , -0.11293831,  0.03515668, -0.02184428,  0.04519583,\n",
       "        0.01256739,  0.08908774,  0.13015911,  0.08408835, -0.08498771,\n",
       "       -0.00520538, -0.06324027,  0.03293116, -0.05643614, -0.07339357,\n",
       "       -0.07496987, -0.03320111,  0.0165253 ,  0.10261366, -0.11041176,\n",
       "        0.03005616,  0.04153862,  0.08357301,  0.00861186,  0.00747747,\n",
       "       -0.01321621,  0.00180046, -0.11988565,  0.01889175,  0.04229261,\n",
       "        0.0306059 ,  0.0995873 ,  0.0048301 , -0.07539985, -0.1056777 ,\n",
       "        0.05742266,  0.09591354,  0.0997717 , -0.00347214, -0.01328623,\n",
       "        0.06518   , -0.02022274,  0.07860503, -0.05283794, -0.05861448,\n",
       "       -0.02421556, -0.01641046, -0.02097617, -0.06931981,  0.00154183,\n",
       "       -0.0952801 ,  0.04769877, -0.03307071,  0.11443636, -0.03122235,\n",
       "        0.05687458, -0.01587497,  0.01220079,  0.08142629,  0.04329115,\n",
       "       -0.05821301,  0.15790352,  0.00144996,  0.01598949, -0.05276173,\n",
       "        0.03781214,  0.04200834, -0.00668729,  0.06347772, -0.09964065,\n",
       "        0.06416804,  0.02288135, -0.04829818,  0.05811761, -0.01938424,\n",
       "        0.1113263 ,  0.04776462, -0.01806725,  0.0906211 ,  0.0050381 ,\n",
       "        0.04421337,  0.04450079, -0.06512292, -0.03584119, -0.03164924,\n",
       "        0.00436038,  0.11536776, -0.06129784,  0.11086916, -0.04571701,\n",
       "       -0.05605824,  0.00684635, -0.01834394, -0.08864667,  0.04717503,\n",
       "        0.10843136, -0.01405843, -0.19439876,  0.06726146, -0.01726108,\n",
       "       -0.01081836,  0.07034668,  0.00346614,  0.02155373,  0.09381367,\n",
       "       -0.00373888, -0.02165523,  0.04319003,  0.02875465,  0.07837599,\n",
       "       -0.00771119, -0.03736502, -0.08864443, -0.03978667, -0.09738614,\n",
       "        0.10562897,  0.11679336,  0.05807592,  0.03067006,  0.05500535,\n",
       "       -0.10243384,  0.0901302 ,  0.06021738, -0.13321191, -0.0916393 ,\n",
       "       -0.08337593, -0.05524301,  0.1344017 ,  0.04950702, -0.06146215,\n",
       "        0.11102771,  0.04961273, -0.03354781,  0.01054018, -0.0293557 ,\n",
       "       -0.05220153,  0.06358465, -0.17176078,  0.00063788, -0.06969393,\n",
       "       -0.0217359 , -0.03895263,  0.0081892 ,  0.08985126, -0.02013075,\n",
       "        0.02552364,  0.04006147,  0.06335739,  0.00933009,  0.01621206,\n",
       "        0.06878159, -0.0027437 , -0.03242287,  0.08327793,  0.04990948,\n",
       "       -0.01947107,  0.06198388,  0.02970986, -0.04007352,  0.02526413,\n",
       "       -0.03466707,  0.01044529,  0.11125578, -0.05534476,  0.0756868 ,\n",
       "       -0.06550513,  0.09938513,  0.02686232, -0.01109676,  0.00464208,\n",
       "        0.01951815, -0.04974073,  0.0705277 , -0.05299592,  0.00780279,\n",
       "       -0.03049673, -0.01394854,  0.09306359,  0.05942754,  0.06981929,\n",
       "       -0.00835966, -0.04472303,  0.02657849,  0.0193628 , -0.00144089,\n",
       "       -0.10252205,  0.05750782, -0.08665457,  0.02733053,  0.08688617,\n",
       "       -0.01985602,  0.02670925,  0.03800858,  0.01895786, -0.0553546 ,\n",
       "        0.05354679, -0.05237507, -0.04009919, -0.04712726, -0.01456667,\n",
       "        0.07129861, -0.0091038 , -0.06104657, -0.08265445], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-albert-small-v2')\n",
    "\n",
    "\n",
    "def encode_into_vector():\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling. In this case, max pooling.\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    print(\"Sentence embeddings:\")\n",
    "    print(sentence_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
